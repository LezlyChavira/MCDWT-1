{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motion Compensated Discrete Wavelet Transform (MCDWT)\n",
    "\n",
    "MCDWT is a [video] decorrelator and [visual] information organizer. The\n",
    "input sequence of pixels_ are [decorrelated] in the [time] and in the\n",
    "[spatial domains]. The output sequence of [coefficients] have a\n",
    "smaller [entropy] than the original pixels, and the information is\n",
    "represented by [resolution levels].\n",
    "\n",
    "Temporal decorrelation is provided by [MC (Motion Compensation)], where the\n",
    "[prediction images] are generated with an algorithm that uses only the\n",
    "information available at the [decoder]. This means that the motion\n",
    "information used in the predictions do not need to be sent to the\n",
    "decoder.\n",
    "\n",
    "Spatial decorrelation is performed by the [analysis filter] used in the\n",
    "[2D-DWT].\n",
    "\n",
    "[video]: https://en.wikipedia.org/wiki/Video\n",
    "[visual]: https://en.wikipedia.org/wiki/Visual_perception\n",
    "[pixels]: https://en.wikipedia.org/wiki/Pixel\n",
    "[decorrelated]: https://en.wikipedia.org/wiki/Decorrelation\n",
    "[time]: https://en.wikipedia.org/wiki/Time_domain\n",
    "[spatial]: https://www.quora.com/What-is-spatial-domain-in-image-processing\n",
    "[coefficients]: https://www.quora.com/What-is-spatial-domain-in-image-processing\n",
    "[entropy]: https://en.wikipedia.org/wiki/Entropy\n",
    "[resolution levels]: https://en.wikipedia.org/wiki/Image_resolution\n",
    "[MC (Motion Compensation)]: https://en.wikipedia.org/wiki/Motion_compensation\n",
    "[decoder]: https://en.wikipedia.org/wiki/Decoder\n",
    "[prediction images]: https://en.wikipedia.org/wiki/Decoder\n",
    "[analysis filter]: https://en.wikipedia.org/wiki/Digital_filter#Analysis_techniques\n",
    "[2D-DWT]: https://en.wikipedia.org/wiki/Discrete_wavelet_transform\n",
    "[EBCOT]: http://nptel.ac.in/courses/117105083/pdf/ssg_m5l15.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video [scalability]\n",
    "\n",
    "MCDWT inputs a [video] and outputs a video, in a way that when using\n",
    "only a portion of the data of the transformed video, a video with a\n",
    "lower [temporal resolution], lower [spatial resolution] or/and lower\n",
    "[quality] can be generated.\n",
    "\n",
    "If all the transformed data is used, then the original video is\n",
    "obtained (MCDWT is a lossless transform). The video output has exactly\n",
    "the same number of elements than the input video (for example, no\n",
    "extra motion fields are produced). At this moment, we will focuse only\n",
    "on spatial [scalability].\n",
    "\n",
    "[temporal resolution]: https://en.wikipedia.org/wiki/Temporal_resolution\n",
    "[spatial resolution]: https://en.wikipedia.org/wiki/Image_resolution#Spatial_resolution\n",
    "[quality]: https://en.wikipedia.org/wiki/Compression_artifact\n",
    "[scalability]: http://inst.eecs.berkeley.edu/~ee290t/sp04/lectures/videowavelet_UCB1-3.pdf\n",
    "[video]: https://en.wikipedia.org/wiki/Video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video transformation alternatives\n",
    "\n",
    "To obtain a multiresolution version or a video, the [DWT] (Discrete\n",
    "Wavelet Transform) can be applied along temporal (t) and\n",
    "spatial domains (2D). At this point, two alternatives\n",
    "arise: (1) a t+2D transform or (2) a 2D+t\n",
    "transform. In a t+2D transform, the video is first analyzed\n",
    "over the time domain and next, over the space domain. A 2D+t\n",
    "transform does just the opposite.\n",
    "\n",
    "[DWT]: https://en.wikipedia.org/wiki/Discrete_wavelet_transform\n",
    "\n",
    "Each choice has a number of *pros* and *cons*. For example, in a\n",
    "t+2D transform we can apply directly any image predictor based\n",
    "on motion estimation because the input is a normal video. However, if\n",
    "we implement a 2D+t transform, the input to the motion\n",
    "estimator is a sequence of images in the DWT domain. [The overwhelming\n",
    "majority of DWT's][Friendly Guide] are not [shift invariant], which basically means\n",
    "$\\text{DWT}(s[t]) \\neq \\text{DWT}(s[t+x])$, where $x$ is a\n",
    "displacement of the $s[t]$ along the time\n",
    "domain. Therefore, motion estimators which compare pixel values will\n",
    "not work on the DWT domain. On the other hand, if we want to provide\n",
    "true spatial scalability (processing only those spatial resolutions\n",
    "(scales) necessary to get a spatially scaled of our video), a\n",
    "t+2D transformed video is not suitable because the first step\n",
    "of the forward transform (t) should be reversed at full\n",
    "resolution in the backward transform (as the forward transform did).\n",
    "\n",
    "[shift invariant]: http://www.polyvalens.com/blog/wavelets/theory\n",
    "[Friendly Guide]: http://www.polyvalens.com/blog/wavelets/theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet and pyramid domains\n",
    "\n",
    "Indeed, the DWT allows to get a scalable representation of a image and by extension, of a video if we apply the DWT on all the images of the video. However, this can be also done with [Gaussian and Laplacian pyramids][Laplacian Pyramids]. Image pyramids are interesting because they are shift invariant and therefore, one can operate within the scales as they are *normal* images. Unfortunately, as a consecuence of pyramids representations are not critically sampled, they need more picture elements than in [Wavelet pyramids][Wavelet Pyramids] and this is a drawback when compressing. Luckily, it is very fast to convert a Laplacian pyramid representation into it DWT equivalent representation, and viceversa. For this reason, even if we use the Wavelet pyramids to work with our images, we can suppose at any moment that we are working with the Laplacian pyramid of those images.\n",
    "\n",
    "[Laplacian Pyramids]: https://en.wikipedia.org/wiki/Pyramid_(image_processing)\n",
    "[Wavelet Pyranids]: http://www.vtvt.ece.vt.edu/research/references/video/DCT_Video_Compression/Zhang92a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The 's'-levels 2D Discrete Wavelet Transform\n",
    "\n",
    "A [2D-DWT][2D-DWT] (2 Dimensions - Discrete Wavelet Transform) generates a scalable representation of an image and by extension, of a video if we apply the DWT on all the images of the video. This is done, for example, in [the JPEG2000 image and video compression standard][J2K]. Notice that only the spatial redundancy is exploited. All the temporal redundancy is still in the video.\n",
    "\n",
    "[J2K]: https://en.wikipedia.org/wiki/JPEG_2000\n",
    "[2D-DWT]: https://en.wikipedia.org/wiki/Discrete_wavelet_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "\n",
    "A sequence `V` of `n` images:\n",
    "\n",
    "```\n",
    "                                                         x\n",
    "+---------------+  +---------------+     +---------------+\n",
    "|               |  |               |     |            |  |\n",
    "|               |  |               |   y |----------- O <---- V[n-1][y][x]\n",
    "|               |  |               | ... |               |\n",
    "|               |  |               |     |               |\n",
    "|               |  |               |     |               |\n",
    "|               |  |               |     |               |\n",
    "+---------------+  +---------------+     +---------------+\n",
    "      V[0]               V[1]                 V[n-1]\n",
    "```\n",
    "\n",
    "### Output\n",
    "\n",
    "A sequence `S` of `n` \"pyramids\". For example, a 2-levels 2D-DWT looks like:\n",
    "\n",
    "```\n",
    "+---+---+-------+  +---+---+-------+     +---+---+-------+\n",
    "|LL2|HL2|       |  |   |   |       |     |   |   |       |\n",
    "+---+---+  HL1  |  +---+---+       |     +---+---+       |\n",
    "|LH2|HH2|       |  |   |   |       |     |   |   |       |\n",
    "+---+---+-------+  +---+---+-------+ ... +---+---+-------+\n",
    "|       |       |  |       |       |     |       |       |\n",
    "|  LH1  |  HH1  |  |       |       |     |       |       |\n",
    "|       |       |  |       |       |     |       |       |        \n",
    "+-------+-------+  +-------+-------+     +-------+-------+\n",
    "S[0]               S[1]                  S[2]\n",
    "```\n",
    "where `L` and `H` stands for *low-pass filtered* and *high-pass filtered*, respectively. The integer > 1 that follows these letters represents the subband level. For the sake of simplicity, we will denote the subbands `{LH, HL, HH}` as only `H`, and `LL` as only `L`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "```pytho\n",
    "for image in V:\n",
    "  2D_DWT(image) # In place\n",
    "S = V # Pointer copy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
